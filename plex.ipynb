{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch transformers networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from typing import List, Dict, Set, Tuple\n",
    "import networkx as nx\n",
    "\n",
    "model_name = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE:\n",
    "    def __init__(self, llm_model: str = \"gpt-3.5-turbo\", kg_path: str = None):\n",
    "        \"\"\"\n",
    "        Initialize GIVE framework\n",
    "        Args:\n",
    "            llm_model: Name of the LLM model to use\n",
    "            kg_path: Path to knowledge graph data\n",
    "        \"\"\"\n",
    "        self.llm = self._initialize_llm(llm_model)\n",
    "        self.kg = self._load_knowledge_graph(kg_path)\n",
    "        self.encoder = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def _initialize_llm(self, model_name: str):\n",
    "        \"\"\"Initialize LLM interface\"\"\"\n",
    "        import openai\n",
    "        from dotenv import load_dotenv\n",
    "        import os\n",
    "        \n",
    "        load_dotenv()\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "            \n",
    "        if \"gpt\" in model_name.lower():\n",
    "            try:\n",
    "                client = openai.Client(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "                # Test the client with a simple request\n",
    "                client.models.list()\n",
    "                return client\n",
    "            except openai.AuthenticationError:\n",
    "                raise ValueError(\"Invalid OpenAI API key\")\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to initialize OpenAI client: {str(e)}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def _load_knowledge_graph(self, kg_path: str) -> nx.Graph:\n",
    "        \"\"\"Load knowledge graph from file\"\"\"\n",
    "        if kg_path is None:\n",
    "            return nx.Graph()\n",
    "            \n",
    "        import rdflib\n",
    "        graph = rdflib.Graph()\n",
    "        file_format = kg_path.split('.')[-1]\n",
    "        \n",
    "        try:\n",
    "            if file_format == 'ttl':\n",
    "                graph.parse(kg_path, format='turtle') \n",
    "            elif file_format in ['jsonld', 'json']:\n",
    "                graph.parse(kg_path, format='json-ld')\n",
    "            elif file_format == 'owl':\n",
    "                graph.parse(kg_path, format='xml')\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file format: {file_format}\")\n",
    "                \n",
    "            # Convert RDF graph to NetworkX graph\n",
    "            nx_graph = nx.Graph()\n",
    "            for s, p, o in graph:\n",
    "                nx_graph.add_edge(str(s), str(o), relation=str(p))\n",
    "                \n",
    "            return nx_graph\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading knowledge graph: {str(e)}\")\n",
    "            return nx.Graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "give = GIVE()\n",
    "graph = give._load_knowledge_graph(\"../data/custinfo_jsonld.jsonld\")\n",
    "assert isinstance(graph, nx.Graph)\n",
    "        # assert len(graph.nodes) > 0, \"Graph should contain nodes\"\n",
    "        # assert len(graph.edges) > 0, \"Graph should contain edges\"\n",
    "        \n",
    "# Print basic graph stats\n",
    "print(f\"Loaded graph with {len(graph.nodes)} nodes and {len(graph.edges)} edges\")\n",
    "        \n",
    "# Sample check of edge properties\n",
    "for u, v, data in list(graph.edges(data=True))[:3]:\n",
    "    print(f\"Edge: {u} -> {v}\")\n",
    "    print(f\"Relation: {data.get('relation')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Tuple\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "class QueryExtraction(BaseModel):\n",
    "    Entities: List[str]\n",
    "    Relations: List[str]\n",
    "\n",
    "class GIVE(GIVE):\n",
    "    def extract_query_info(self, query: str) -> Tuple[List[str], List[str]]:\n",
    "        prompt = \"\"\"Extract key entities and relations from this query in JSON format:\n",
    "        {\n",
    "            \"Entities\": [\"entity1\", \"entity2\"],\n",
    "            \"Relations\": [\"relation1\", \"relation2\"]\n",
    "        }\"\"\"\n",
    "        \n",
    "        client = OpenAI()\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt + \"\\n\\nQuery: \" + query}\n",
    "            ],\n",
    "            response_format=QueryExtraction,\n",
    "        )\n",
    "        \n",
    "        response = completion.choices[0].message.parsed\n",
    "        entities = response.Entities\n",
    "        relations = response.Relations\n",
    "        \n",
    "        return entities, relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "give = GIVE()\n",
    "entities, relations = give.extract_query_info(\"What is the relationship between John Doe and Jane Smith?\")\n",
    "print(entities)\n",
    "print(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def _get_similar_entities(self, encoded_query: torch.Tensor, top_p: int = 5) -> List[str]:\n",
    "        \"\"\"Find similar entities in knowledge graph using cosine similarity\"\"\"\n",
    "        similar_entities = []\n",
    "        \n",
    "        # Get all entities from the knowledge graph\n",
    "        entities = list(self.kg.nodes())\n",
    "        if not entities:\n",
    "            return similar_entities\n",
    "            \n",
    "        # Encode all entities\n",
    "        with torch.no_grad():\n",
    "            entity_encodings = torch.cat([self._encode_text(e) for e in entities])\n",
    "            \n",
    "        # Calculate cosine similarity\n",
    "        similarities = torch.nn.functional.cosine_similarity(\n",
    "            encoded_query, entity_encodings\n",
    "        )\n",
    "        \n",
    "        # Get top-p similar entities\n",
    "        top_indices = torch.argsort(similarities, descending=True)[:top_p]\n",
    "        similar_entities = [entities[i] for i in top_indices]\n",
    "        \n",
    "        return similar_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def construct_entity_groups(self, entities: List[str], top_p: int = 5) -> Dict[str, Set[str]]:\n",
    "        \"\"\"\n",
    "        Construct groups of similar entities\n",
    "        Args:\n",
    "            entities: List of query entities\n",
    "            top_p: Number of similar entities to retrieve\n",
    "        Returns:\n",
    "            Dictionary mapping query entities to sets of similar entities\n",
    "        \"\"\"\n",
    "        entity_groups = {}\n",
    "        for entity in entities:\n",
    "            # Encode query entity\n",
    "            encoded = self._encode_text(entity)\n",
    "            \n",
    "            # Find similar entities in KG\n",
    "            similar_entities = self._get_similar_entities(encoded, top_p)\n",
    "            entity_groups[entity] = set(similar_entities)\n",
    "            \n",
    "        return entity_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def _encode_text(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"Encode text using pretrained encoder\"\"\"\n",
    "        tokens = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            output = self.encoder(**tokens)\n",
    "        return output.last_hidden_state.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def induce_inner_group_connections(self, entity_groups: Dict[str, Set[str]]) -> List[Tuple]:\n",
    "        \"\"\"\n",
    "        Induce connections within entity groups\n",
    "        Args:\n",
    "            entity_groups: Dictionary of entity groups\n",
    "        Returns:\n",
    "            List of (entity1, relation, entity2) triplets\n",
    "        \"\"\"\n",
    "        connections = []\n",
    "        for entity, group in entity_groups.items():\n",
    "            for similar_entity in group:\n",
    "                relation = self._get_llm_relation(entity, similar_entity)\n",
    "                if relation:\n",
    "                    connections.append((entity, relation, similar_entity))\n",
    "        return connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def discover_intermediate_groups(self, \n",
    "                                  source_group: Set[str], \n",
    "                                  target_group: Set[str]) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Discover intermediate node groups for multi-hop reasoning\n",
    "        Args:\n",
    "            source_group: Source entity group\n",
    "            target_group: Target entity group\n",
    "        Returns:\n",
    "            Set of intermediate entities\n",
    "        \"\"\"\n",
    "        # Find length-2 paths between groups\n",
    "        paths = self._find_paths(source_group, target_group, max_length=2)\n",
    "        \n",
    "        # Use LLM to select most relevant intermediate nodes\n",
    "        intermediate_nodes = self._select_intermediate_nodes(paths)\n",
    "        \n",
    "        # Construct intermediate group\n",
    "        return self.construct_entity_groups([intermediate_nodes])[intermediate_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def _extract_kg_knowledge(self, entity_groups: Dict[str, Set[str]]) -> List[Tuple]:\n",
    "        \"\"\"Extract knowledge from knowledge graph\"\"\"\n",
    "        knowledge = []\n",
    "        \n",
    "        # Flatten all entities from groups\n",
    "        all_entities = {e for group in entity_groups.values() for e in group}\n",
    "        \n",
    "        # Get all edges between entities in groups\n",
    "        for e1 in all_entities:\n",
    "            for e2 in all_entities:\n",
    "                if self.kg.has_edge(e1, e2):\n",
    "                    edge_data = self.kg.get_edge_data(e1, e2)\n",
    "                    knowledge.append((e1, edge_data['relation'], e2))\n",
    "        \n",
    "        return knowledge\n",
    "\n",
    "    def _extrapolate_llm_knowledge(self, entity_groups: Dict[str, Set[str]], \n",
    "                                 potential_relations: Set[str]) -> List[Tuple]:\n",
    "        \"\"\"Use LLM to extrapolate additional knowledge\"\"\"\n",
    "        knowledge = []\n",
    "        \n",
    "        for entity, similar_entities in entity_groups.items():\n",
    "            for similar_entity in similar_entities:\n",
    "                prompt = f\"What is the relationship between {entity} and {similar_entity}?\"\n",
    "                response = self.llm.chat.completions.create(\n",
    "                    model=model_name,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                relation = response.choices[0].message.content.strip()\n",
    "                if relation in potential_relations:\n",
    "                    knowledge.append((entity, relation, similar_entity))\n",
    "        \n",
    "        return knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def extrapolate_knowledge(self, \n",
    "                            entity_groups: Dict[str, Set[str]], \n",
    "                            potential_relations: Set[str]) -> List[Tuple]:\n",
    "        \"\"\"\n",
    "        Extrapolate knowledge using LLM and KG structure\n",
    "        Args:\n",
    "            entity_groups: Dictionary of entity groups\n",
    "            potential_relations: Set of potential relations\n",
    "        Returns:\n",
    "            List of (subject, relation, object) triplets\n",
    "        \"\"\"\n",
    "        knowledge = []\n",
    "        \n",
    "        # Extract knowledge from KG\n",
    "        kg_knowledge = self._extract_kg_knowledge(entity_groups)\n",
    "        knowledge.extend(kg_knowledge)\n",
    "        \n",
    "        # Extrapolate using LLM\n",
    "        extrapolated = self._extrapolate_llm_knowledge(entity_groups, potential_relations)\n",
    "        knowledge.extend(extrapolated)\n",
    "        \n",
    "        return knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def _is_affirmative(self, knowledge_tuple: Tuple) -> bool:\n",
    "        \"\"\"Check if knowledge tuple represents affirmative knowledge\"\"\"\n",
    "        # Simple implementation - could be enhanced with more sophisticated logic\n",
    "        return True  # For now, treat all knowledge as affirmative\n",
    "    \n",
    "    def _generate_with_knowledge(self, query: str, knowledge: List[Tuple]) -> str:\n",
    "        \"\"\"Generate answer using provided knowledge\"\"\"\n",
    "        # Format knowledge into readable text\n",
    "        knowledge_text = \"\\n\".join([f\"{s} {r} {o}\" for s, r, o in knowledge])\n",
    "        \n",
    "        prompt = f\"\"\"Given this knowledge:\n",
    "{knowledge_text}\n",
    "\n",
    "Answer this question: {query}\"\"\"\n",
    "        \n",
    "        response = self.llm.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def generate_answer(self, query: str, knowledge: List[Tuple]) -> str:\n",
    "        \"\"\"\n",
    "        Generate final answer using progressive refinement\n",
    "        Args:\n",
    "            query: Input query\n",
    "            knowledge: List of knowledge triplets\n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Split knowledge into affirmative and counter-factual\n",
    "        affirmative = [k for k in knowledge if self._is_affirmative(k)]\n",
    "        counterfactual = [k for k in knowledge if not self._is_affirmative(k)]\n",
    "        \n",
    "        # Progressive refinement\n",
    "        initial_answer = self._generate_with_knowledge(query, affirmative)\n",
    "        refined_answer = self._generate_with_knowledge(query, affirmative + counterfactual)\n",
    "        final_answer = self._generate_with_knowledge(query, knowledge)\n",
    "        \n",
    "        return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GIVE\n",
    "give = GIVE(llm_model=\"gpt-4o\", kg_path=\"../data/CustomerInfo.owl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John Doe', 'Jane Smith']\n",
      "['relationship']\n"
     ]
    }
   ],
   "source": [
    "# Process a query\n",
    "query = \"What is the relationship between John Doe and Jane Smith?\"\n",
    "\n",
    "# Extract query information\n",
    "entities, relations = give.extract_query_info(query)\n",
    "\n",
    "print(entities)\n",
    "print(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'John Doe': {'42', 'http://www.w3.org/2002/07/owl#NamedIndividual', 'http://www.semanticweb.org/evishga/ontologies/2019/0/untitled-ontology-62#ProbableCustomer', 'Nea86ac8fc4544d17a6d27a5780620476', 'Nc44b12a7dea7416990f96720289646f6'}, 'Jane Smith': {'42', 'Na2ac1e4133ba4398a21a5027dccd3097', 'http://www.semanticweb.org/evishga/ontologies/2019/0/untitled-ontology-62#ProbableCustomer', 'N19689ff105464942a381c63a2649e606', 'Na7fe68c675b5466694cd0014fea1a9e0'}}\n"
     ]
    }
   ],
   "source": [
    "# Construct entity groups\n",
    "entity_groups = give.construct_entity_groups(entities)\n",
    "\n",
    "print(entity_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Generate knowledge\n",
    "knowledge = give.extrapolate_knowledge(entity_groups, relations)\n",
    "\n",
    "print(knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I can't provide information on the relationship between John Doe and Jane Smith without more context. These names are often used as placeholders or examples, so more specific details would be needed to understand any relationship they might have.\n"
     ]
    }
   ],
   "source": [
    "# Generate answer\n",
    "answer = give.generate_answer(query, knowledge)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GIVE\n",
    "give = GIVE(llm_model=\"gpt-4o\", kg_path=\"../data/custinfo_jsonld.jsonld\")\n",
    "\n",
    "# Process a query\n",
    "query = \"What is the relationship between John Doe and Jane Smith?\"\n",
    "\n",
    "# Extract query information\n",
    "entities, relations = give.extract_query_info(query)\n",
    "\n",
    "# Construct entity groups\n",
    "entity_groups = give.construct_entity_groups(entities)\n",
    "\n",
    "# Generate knowledge\n",
    "knowledge = give.extrapolate_knowledge(entity_groups, relations)\n",
    "\n",
    "# Generate answer\n",
    "answer = give.generate_answer(query, knowledge)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
