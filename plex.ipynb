{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.2.2-cp312-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/rohhh/repo/researchPaper/.venv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/rohhh/repo/researchPaper/.venv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/rohhh/repo/researchPaper/.venv/lib/python3.12/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/rohhh/repo/researchPaper/.venv/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/rohhh/repo/researchPaper/.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/rohhh/repo/researchPaper/.venv/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /Users/rohhh/repo/researchPaper/.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-cp312-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp312-cp312-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/rohhh/repo/researchPaper/.venv/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rohhh/repo/researchPaper/.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rohhh/repo/researchPaper/.venv/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rohhh/repo/researchPaper/.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rohhh/repo/researchPaper/.venv/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rohhh/repo/researchPaper/.venv/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached torch-2.2.2-cp312-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "Downloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading safetensors-0.4.5-cp312-cp312-macosx_10_12_x86_64.whl (392 kB)\n",
      "Downloading tokenizers-0.20.3-cp312-cp312-macosx_10_12_x86_64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, safetensors, networkx, fsspec, filelock, torch, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.10.0 huggingface-hub-0.26.2 mpmath-1.3.0 networkx-3.4.2 safetensors-0.4.5 sympy-1.13.3 tokenizers-0.20.3 torch-2.2.2 transformers-4.46.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install torch transformers networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from typing import List, Dict, Set, Tuple\n",
    "import networkx as nx\n",
    "\n",
    "model_name = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE:\n",
    "    def __init__(self, llm_model: str = \"gpt-3.5-turbo\", kg_path: str = None):\n",
    "        \"\"\"\n",
    "        Initialize GIVE framework\n",
    "        Args:\n",
    "            llm_model: Name of the LLM model to use\n",
    "            kg_path: Path to knowledge graph data\n",
    "        \"\"\"\n",
    "        self.llm = self._initialize_llm(llm_model)\n",
    "        self.kg = self._load_knowledge_graph(kg_path)\n",
    "        self.encoder = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def _initialize_llm(self, model_name: str):\n",
    "        \"\"\"Initialize LLM interface\"\"\"\n",
    "        import openai\n",
    "        from dotenv import load_dotenv\n",
    "        import os\n",
    "        \n",
    "        load_dotenv()\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "            \n",
    "        if \"gpt\" in model_name.lower():\n",
    "            try:\n",
    "                client = openai.Client(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "                # Test the client with a simple request\n",
    "                client.models.list()\n",
    "                return client\n",
    "            except openai.AuthenticationError:\n",
    "                raise ValueError(\"Invalid OpenAI API key\")\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to initialize OpenAI client: {str(e)}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def _load_knowledge_graph(self, kg_path: str) -> nx.Graph:\n",
    "        \"\"\"Load knowledge graph from file\"\"\"\n",
    "        if kg_path is None:\n",
    "            return nx.Graph()\n",
    "            \n",
    "        import rdflib\n",
    "        graph = rdflib.Graph()\n",
    "        file_format = kg_path.split('.')[-1]\n",
    "        \n",
    "        try:\n",
    "            if file_format == 'ttl':\n",
    "                graph.parse(kg_path, format='turtle') \n",
    "            elif file_format in ['jsonld', 'json']:\n",
    "                graph.parse(kg_path, format='json-ld')\n",
    "            elif file_format == 'owl':\n",
    "                graph.parse(kg_path, format='xml')\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file format: {file_format}\")\n",
    "                \n",
    "            # Convert RDF graph to NetworkX graph\n",
    "            nx_graph = nx.Graph()\n",
    "            for s, p, o in graph:\n",
    "                nx_graph.add_edge(str(s), str(o), relation=str(p))\n",
    "                \n",
    "            return nx_graph\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading knowledge graph: {str(e)}\")\n",
    "            return nx.Graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded graph with 418 nodes and 925 edges\n",
      "Edge: N8d7fb8f2e1af404fb06a67fb3dc9e876 -> N2fbd773c5fa84d25907edad3b9562040\n",
      "Relation: http://www.w3.org/1999/02/22-rdf-syntax-ns#rest\n",
      "Edge: N8d7fb8f2e1af404fb06a67fb3dc9e876 -> http://www.semanticweb.org/evishga/ontologies/2019/0/untitled-ontology-62#Celebrity\n",
      "Relation: http://www.w3.org/1999/02/22-rdf-syntax-ns#first\n",
      "Edge: N8d7fb8f2e1af404fb06a67fb3dc9e876 -> N4e6addcd8978450d987d4f7722fb8a4a\n",
      "Relation: http://www.w3.org/1999/02/22-rdf-syntax-ns#rest\n"
     ]
    }
   ],
   "source": [
    "give = GIVE()\n",
    "graph = give._load_knowledge_graph(\"../data/custinfo_jsonld.jsonld\")\n",
    "assert isinstance(graph, nx.Graph)\n",
    "        # assert len(graph.nodes) > 0, \"Graph should contain nodes\"\n",
    "        # assert len(graph.edges) > 0, \"Graph should contain edges\"\n",
    "        \n",
    "# Print basic graph stats\n",
    "print(f\"Loaded graph with {len(graph.nodes)} nodes and {len(graph.edges)} edges\")\n",
    "        \n",
    "# Sample check of edge properties\n",
    "for u, v, data in list(graph.edges(data=True))[:3]:\n",
    "    print(f\"Edge: {u} -> {v}\")\n",
    "    print(f\"Relation: {data.get('relation')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Tuple\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "class QueryExtraction(BaseModel):\n",
    "    Entities: List[str]\n",
    "    Relations: List[str]\n",
    "\n",
    "class GIVE(GIVE):\n",
    "    def extract_query_info(self, query: str) -> Tuple[List[str], List[str]]:\n",
    "        prompt = \"\"\"Extract key entities and relations from this query in JSON format:\n",
    "        {\n",
    "            \"Entities\": [\"entity1\", \"entity2\"],\n",
    "            \"Relations\": [\"relation1\", \"relation2\"]\n",
    "        }\"\"\"\n",
    "        \n",
    "        client = OpenAI()\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt + \"\\n\\nQuery: \" + query}\n",
    "            ],\n",
    "            response_format=QueryExtraction,\n",
    "        )\n",
    "        \n",
    "        response = completion.choices[0].message.parsed\n",
    "        entities = response.Entities\n",
    "        relations = response.Relations\n",
    "        \n",
    "        return entities, relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John Doe', 'Jane Smith']\n",
      "['relationship between']\n"
     ]
    }
   ],
   "source": [
    "give = GIVE()\n",
    "entities, relations = give.extract_query_info(\"What is the relationship between John Doe and Jane Smith?\")\n",
    "print(entities)\n",
    "print(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def _get_similar_entities(self, encoded_query: torch.Tensor, top_p: int = 5) -> List[str]:\n",
    "        \"\"\"Find similar entities in knowledge graph using cosine similarity\"\"\"\n",
    "        similar_entities = []\n",
    "        \n",
    "        # Get all entities from the knowledge graph\n",
    "        entities = list(self.kg.nodes())\n",
    "        if not entities:\n",
    "            return similar_entities\n",
    "            \n",
    "        # Encode all entities\n",
    "        with torch.no_grad():\n",
    "            entity_encodings = torch.cat([self._encode_text(e) for e in entities])\n",
    "            \n",
    "        # Calculate cosine similarity\n",
    "        similarities = torch.nn.functional.cosine_similarity(\n",
    "            encoded_query, entity_encodings\n",
    "        )\n",
    "        \n",
    "        # Get top-p similar entities\n",
    "        top_indices = torch.argsort(similarities, descending=True)[:top_p]\n",
    "        similar_entities = [entities[i] for i in top_indices]\n",
    "        \n",
    "        return similar_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def construct_entity_groups(self, entities: List[str], top_p: int = 5) -> Dict[str, Set[str]]:\n",
    "        \"\"\"\n",
    "        Construct groups of similar entities\n",
    "        Args:\n",
    "            entities: List of query entities\n",
    "            top_p: Number of similar entities to retrieve\n",
    "        Returns:\n",
    "            Dictionary mapping query entities to sets of similar entities\n",
    "        \"\"\"\n",
    "        entity_groups = {}\n",
    "        for entity in entities:\n",
    "            # Encode query entity\n",
    "            encoded = self._encode_text(entity)\n",
    "            \n",
    "            # Find similar entities in KG\n",
    "            similar_entities = self._get_similar_entities(encoded, top_p)\n",
    "            entity_groups[entity] = set(similar_entities)\n",
    "            \n",
    "        return entity_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def _encode_text(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"Encode text using pretrained encoder\"\"\"\n",
    "        tokens = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            output = self.encoder(**tokens)\n",
    "        return output.last_hidden_state.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def induce_inner_group_connections(self, entity_groups: Dict[str, Set[str]]) -> List[Tuple]:\n",
    "        \"\"\"\n",
    "        Induce connections within entity groups\n",
    "        Args:\n",
    "            entity_groups: Dictionary of entity groups\n",
    "        Returns:\n",
    "            List of (entity1, relation, entity2) triplets\n",
    "        \"\"\"\n",
    "        connections = []\n",
    "        for entity, group in entity_groups.items():\n",
    "            for similar_entity in group:\n",
    "                relation = self._get_llm_relation(entity, similar_entity)\n",
    "                if relation:\n",
    "                    connections.append((entity, relation, similar_entity))\n",
    "        return connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def discover_intermediate_groups(self, \n",
    "                                  source_group: Set[str], \n",
    "                                  target_group: Set[str]) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Discover intermediate node groups for multi-hop reasoning\n",
    "        Args:\n",
    "            source_group: Source entity group\n",
    "            target_group: Target entity group\n",
    "        Returns:\n",
    "            Set of intermediate entities\n",
    "        \"\"\"\n",
    "        # Find length-2 paths between groups\n",
    "        paths = self._find_paths(source_group, target_group, max_length=2)\n",
    "        \n",
    "        # Use LLM to select most relevant intermediate nodes\n",
    "        intermediate_nodes = self._select_intermediate_nodes(paths)\n",
    "        \n",
    "        # Construct intermediate group\n",
    "        return self.construct_entity_groups([intermediate_nodes])[intermediate_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def _extract_kg_knowledge(self, entity_groups: Dict[str, Set[str]]) -> List[Tuple]:\n",
    "        \"\"\"Extract knowledge from knowledge graph\"\"\"\n",
    "        knowledge = []\n",
    "        \n",
    "        # Flatten all entities from groups\n",
    "        all_entities = {e for group in entity_groups.values() for e in group}\n",
    "        \n",
    "        # Get all edges between entities in groups\n",
    "        for e1 in all_entities:\n",
    "            for e2 in all_entities:\n",
    "                if self.kg.has_edge(e1, e2):\n",
    "                    edge_data = self.kg.get_edge_data(e1, e2)\n",
    "                    knowledge.append((e1, edge_data['relation'], e2))\n",
    "        \n",
    "        return knowledge\n",
    "\n",
    "    def _extrapolate_llm_knowledge(self, entity_groups: Dict[str, Set[str]], \n",
    "                                 potential_relations: Set[str]) -> List[Tuple]:\n",
    "        \"\"\"Use LLM to extrapolate additional knowledge\"\"\"\n",
    "        knowledge = []\n",
    "        \n",
    "        for entity, similar_entities in entity_groups.items():\n",
    "            for similar_entity in similar_entities:\n",
    "                prompt = f\"What is the relationship between {entity} and {similar_entity}?\"\n",
    "                response = self.llm.chat.completions.create(\n",
    "                    model=model_name,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                relation = response.choices[0].message.content.strip()\n",
    "                if relation in potential_relations:\n",
    "                    knowledge.append((entity, relation, similar_entity))\n",
    "        \n",
    "        return knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def extrapolate_knowledge(self, \n",
    "                            entity_groups: Dict[str, Set[str]], \n",
    "                            potential_relations: Set[str]) -> List[Tuple]:\n",
    "        \"\"\"\n",
    "        Extrapolate knowledge using LLM and KG structure\n",
    "        Args:\n",
    "            entity_groups: Dictionary of entity groups\n",
    "            potential_relations: Set of potential relations\n",
    "        Returns:\n",
    "            List of (subject, relation, object) triplets\n",
    "        \"\"\"\n",
    "        knowledge = []\n",
    "        \n",
    "        # Extract knowledge from KG\n",
    "        kg_knowledge = self._extract_kg_knowledge(entity_groups)\n",
    "        knowledge.extend(kg_knowledge)\n",
    "        \n",
    "        # Extrapolate using LLM\n",
    "        extrapolated = self._extrapolate_llm_knowledge(entity_groups, potential_relations)\n",
    "        knowledge.extend(extrapolated)\n",
    "        \n",
    "        return knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def _is_affirmative(self, knowledge_tuple: Tuple) -> bool:\n",
    "        \"\"\"Check if knowledge tuple represents affirmative knowledge\"\"\"\n",
    "        # Simple implementation - could be enhanced with more sophisticated logic\n",
    "        return True  # For now, treat all knowledge as affirmative\n",
    "    \n",
    "    def _generate_with_knowledge(self, query: str, knowledge: List[Tuple]) -> str:\n",
    "        \"\"\"Generate answer using provided knowledge\"\"\"\n",
    "        # Format knowledge into readable text\n",
    "        knowledge_text = \"\\n\".join([f\"{s} {r} {o}\" for s, r, o in knowledge])\n",
    "        \n",
    "        prompt = f\"\"\"Given this knowledge:\n",
    "{knowledge_text}\n",
    "\n",
    "Answer this question: {query}\"\"\"\n",
    "        \n",
    "        response = self.llm.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIVE(GIVE):\n",
    "    def generate_answer(self, query: str, knowledge: List[Tuple]) -> str:\n",
    "        \"\"\"\n",
    "        Generate final answer using progressive refinement\n",
    "        Args:\n",
    "            query: Input query\n",
    "            knowledge: List of knowledge triplets\n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Split knowledge into affirmative and counter-factual\n",
    "        affirmative = [k for k in knowledge if self._is_affirmative(k)]\n",
    "        counterfactual = [k for k in knowledge if not self._is_affirmative(k)]\n",
    "        \n",
    "        # Progressive refinement\n",
    "        initial_answer = self._generate_with_knowledge(query, affirmative)\n",
    "        refined_answer = self._generate_with_knowledge(query, affirmative + counterfactual)\n",
    "        final_answer = self._generate_with_knowledge(query, knowledge)\n",
    "        \n",
    "        return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GIVE\n",
    "give = GIVE(llm_model=\"gpt-4o\", kg_path=\"../data/custinfo_jsonld.jsonld\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John Doe', 'Jane Smith']\n",
      "['relationship']\n"
     ]
    }
   ],
   "source": [
    "# Process a query\n",
    "query = \"What is the relationship between John Doe and Jane Smith?\"\n",
    "\n",
    "# Extract query information\n",
    "entities, relations = give.extract_query_info(query)\n",
    "\n",
    "print(entities)\n",
    "print(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'John Doe': {'genid11', 'genid41', 'genid51', 'genid22', 'genid204'}, 'Jane Smith': {'genid198', 'genid195', 'genid191', 'genid49', 'genid185'}}\n"
     ]
    }
   ],
   "source": [
    "# Construct entity groups\n",
    "entity_groups = give.construct_entity_groups(entities)\n",
    "\n",
    "print(entity_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Generate knowledge\n",
    "knowledge = give.extrapolate_knowledge(entity_groups, relations)\n",
    "\n",
    "print(knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have the ability to determine the relationship between John Doe and Jane Smith. Their names are generic placeholders often used for anonymous or hypothetical persons, and without specific context or additional information, their relationship cannot be established.\n"
     ]
    }
   ],
   "source": [
    "# Initialize GIVE\n",
    "give = GIVE(llm_model=\"gpt-4o\", kg_path=\"../data/custinfo_jsonld.jsonld\")\n",
    "\n",
    "# Process a query\n",
    "query = \"What is the relationship between John Doe and Jane Smith?\"\n",
    "\n",
    "# Extract query information\n",
    "entities, relations = give.extract_query_info(query)\n",
    "\n",
    "# Construct entity groups\n",
    "entity_groups = give.construct_entity_groups(entities)\n",
    "\n",
    "# Generate knowledge\n",
    "knowledge = give.extrapolate_knowledge(entity_groups, relations)\n",
    "\n",
    "# Generate answer\n",
    "answer = give.generate_answer(query, knowledge)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
